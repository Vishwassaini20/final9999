# -*- coding: utf-8 -*-
"""autolysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1krFZPNOe_TOg1oeoCVXlt_q79kBx1zWT
"""

# /// script
# requires-python = ">=3.12"
# dependencies = [
#     "chardet",
#     "matplotlib",
#     "pandas",
#     "statsmodels",
#     "scikit-learn",
#     "missingno",
#     "python-dotenv",
#     "requests",
#     "seaborn",
# ]
# ///


## This script does generic analysis which includes summarization, cluster analysis, Correlation analysis,
## outlier analysis along with Visulization using python of any csv files and results of this analysis is shared with LLM Model
## to come up with a story and the results are stored a README.md file

# Updated Python Script for Enhanced Analysis
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import missingno as msno
from scipy import stats
import requests
import os
import chardet

def detect_encoding(filename):
    with open(filename, 'rb') as f:
        result = chardet.detect(f.read())
    return result['encoding']

def load_and_clean_data(filename):
    encoding = detect_encoding(filename)
    df = pd.read_csv(filename, encoding=encoding)
    df.dropna(axis=0, how='all', inplace=True)
    numeric_columns = df.select_dtypes(include='number')
    df[numeric_columns.columns] = numeric_columns.fillna(numeric_columns.mean())
    non_numeric_columns = df.select_dtypes(exclude='number')
    df[non_numeric_columns.columns] = non_numeric_columns.fillna('Unknown')
    return df

def summarize_data(df):
    return {
        'shape': df.shape,
        'columns': df.columns.tolist(),
        'types': df.dtypes.to_dict(),
        'descriptive_statistics': df.describe(include='all').to_dict(),
        'missing_values': df.isnull().sum().to_dict()
    }

def detect_outliers(df):
    numeric_df = df.select_dtypes(include=[np.number])
    z_scores = np.abs(stats.zscore(numeric_df))
    return {column: int((z_scores[:, idx] > 3).sum()) 
            for idx, column in enumerate(numeric_df.columns)}

def correlation_analysis(df):
    numeric_df = df.select_dtypes(include='number')
    return numeric_df.corr().to_dict()

def perform_clustering(df, n_clusters=3):
    scaler = StandardScaler()
    df_scaled = scaler.fit_transform(df.select_dtypes(include=[np.number]))
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    df['Cluster'] = kmeans.fit_predict(df_scaled)
    return df, kmeans

def perform_pca(df):
    scaler = StandardScaler()
    df_scaled = scaler.fit_transform(df.select_dtypes(include=[np.number]))
    pca = PCA(n_components=2)
    components = pca.fit_transform(df_scaled)
    df['PCA1'], df['PCA2'] = components[:, 0], components[:, 1]
    return df

def create_visualizations(df):
    numeric_df = df.select_dtypes(include='number')
    visualizations = []

    # Missing data visualization
    msno.matrix(df)
    plt.title('Missing Data Visualization')
    plt.savefig('missing_data.png')
    visualizations.append('missing_data.png')
    plt.close()

    # Correlation heatmap
    if len(numeric_df.columns) > 1:
        plt.figure(figsize=(10, 6))
        sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm')
        plt.title('Correlation Matrix')
        plt.savefig('correlation_matrix.png')
        visualizations.append('correlation_matrix.png')
        plt.close()

    # Histograms for numeric features
    numeric_df.hist(figsize=(12, 10), bins=20)
    plt.suptitle('Distributions of Numeric Columns')
    plt.savefig('numeric_histograms.png')
    visualizations.append('numeric_histograms.png')
    plt.close()

    # Pair plot for numeric relationships
    if len(numeric_df.columns) > 1:
        sns.pairplot(numeric_df.sample(min(500, len(numeric_df))))
        plt.savefig('pairplot.png')
        visualizations.append('pairplot.png')
        plt.close()

    # Cluster visualization
    if 'PCA1' in df.columns and 'PCA2' in df.columns:
        plt.figure(figsize=(8, 6))
        sns.scatterplot(x='PCA1', y='PCA2', hue='Cluster', data=df, palette='Set1')
        plt.title('Cluster Visualization (PCA)')
        plt.savefig('cluster_visualization.png')
        visualizations.append('cluster_visualization.png')
        plt.close()

    return visualizations

def generate_analysis_story(summary, outliers, correlation_matrix, visualizations):
    prompt = f"""
    Below is a detailed summary and analysis of a dataset. Please generate a **rich and engaging narrative** about this dataset analysis, including:

    1. **The Data Received**: Describe the dataset vividly. What does the data represent? What are its features? What is the significance of this data? Create a compelling story around it.
    2. **The Analysis Carried Out**: Explain the analysis methods used. Highlight techniques like missing value handling, outlier detection, clustering, and dimensionality reduction (PCA). How do these methods provide insights?
    3. **Key Insights and Discoveries**: What were the major findings? What trends or patterns emerged that can be interpreted as discoveries? Were there any unexpected results?
    4. **Implications and Actions**: Discuss the implications of these findings. How do they influence decisions? What actionable recommendations would you provide based on the analysis?
    5. **Visualizations**: Describe the visualizations included. What do they reveal about the data? How do they complement the analysis and findings?

    **Dataset Summary**:
    {summary}

    **Outliers**:
    {outliers}

    **Correlation Analysis**:
    {correlation_matrix}

    **Visualizations**:
    {visualizations}
    """
    try:
        response = requests.post(
            "http://aiproxy.sanand.workers.dev/openai/v1/chat/completions",
            headers={"Authorization": f"Bearer {os.environ['AIPROXY_TOKEN']}"},
            json={"model": "gpt-4o-mini", "messages": [{"role": "system", "content": prompt}]}
        )
        story = response.json()["choices"][0]["message"]["content"]
    except Exception as e:
        story = f"Error generating narrative: {e}"
    return story

def write_readme(summary, outliers, correlation_matrix, visualizations, story, filename):
    with open('README.md', 'w') as f:
        f.write(f"# Dataset Analysis of {filename}\n")
        f.write("\n## Dataset Summary\n")
        f.write(str(summary))
        f.write("\n\n## Outlier Detection\n")
        f.write(str(outliers))
        f.write("\n\n## Correlation Analysis\n")
        f.write(str(correlation_matrix))
        f.write("\n\n## Analysis Narrative\n")
        f.write(story)
        f.write("\n\n## Visualizations\n")
        for img in visualizations:
            f.write(f"![{img}]({img})\n")

def main(filename):
    df = load_and_clean_data(filename)
    summary = summarize_data(df)
    outliers = detect_outliers(df)
    correlation_matrix = correlation_analysis(df)
    df, _ = perform_clustering(df)
    df = perform_pca(df)
    visualizations = create_visualizations(df)
    story = generate_analysis_story(summary, outliers, correlation_matrix, visualizations)
    write_readme(summary, outliers, correlation_matrix, visualizations, story, filename)
    print("Analysis complete! Results saved in README.md")

if __name__ == "__main__":
    import sys
    if len(sys.argv) != 2:
        print("Usage: python autolysis.py <dataset.csv>")
    else:
        main(sys.argv[1])
